{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db880d9-0971-490e-a1ba-aad66e744548",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'else' statement on line 217 (2186840138.py, line 219)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 219\u001b[0;36m\u001b[0m\n\u001b[0;31m    layer_idx = 0  # 用于跟踪线性层的索引\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'else' statement on line 217\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from numpy.random import randn\n",
    "import os\n",
    "import cupy as cp\n",
    "import json\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self,Din,Dout):#upstream:从上游传下来的梯度\n",
    "        \n",
    "        self.Din=Din\n",
    "        self.Dout=Dout\n",
    "\n",
    "        \n",
    "        self.W = cp.random.randn(Din, Dout) * cp.sqrt(2. / Din)# He初始化：权重乘以sqrt(2/Din)以控制方差\n",
    "        self.b=cp.zeros((1,self.Dout))#常见错误，不能写成np.zeros(1,self.Dout)，一定注意有两层括号\n",
    "        self.grad_W = 0\n",
    "        self.grad_b = 0\n",
    "        self.cache = None  # 存储前向传播的输入\n",
    "    \n",
    "    def forward(self,X):\n",
    "        self.cache=X\n",
    "        return X@self.W+self.b\n",
    "\n",
    "    def backward(self,upstream):#我们要计算loss对于W，b的偏导，存在本层以及loss对于X的偏导，这是上一层的output\n",
    "        X=self.cache\n",
    "        #out=X*W+b,upstream=偏loss/偏out，下面我们要计算偏loss/偏W，也就是计算偏out/偏W\n",
    "        self.grad_W=X.T@upstream\n",
    "        self.grad_b=cp.sum(upstream, axis=0, keepdims=True)\n",
    "        return upstream @ self.W.T  # grad是dL/dY，self.W是当前层的权重矩阵\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = X\n",
    "        return cp.maximum(0, X)\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        X = self.cache\n",
    "        return upstream * (X > 0)\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        output = 1 / (1 + cp.exp(-X))\n",
    "        self.cache = output\n",
    "        return output\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        output = self.cache\n",
    "        return upstream * output * (1 - output)\n",
    "\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        output = cp.tanh(X)\n",
    "        self.cache = output\n",
    "        return output\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        output = self.cache\n",
    "        return upstream * (1 - output ** 2)\n",
    "\n",
    "class LeakyReLU:\n",
    "    def __init__(self, negative_slope=0.01):\n",
    "        self.cache = None\n",
    "        self.negative_slope = negative_slope\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = X\n",
    "        return cp.where(X > 0, X, self.negative_slope * X)\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        X = self.cache\n",
    "        dX = cp.where(X > 0, 1, self.negative_slope)\n",
    "        return upstream * dX\n",
    "\n",
    "class ELU:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.cache = None\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = X\n",
    "        return cp.where(X > 0, X, self.alpha * (cp.exp(X) - 1))\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        X = self.cache\n",
    "        mask = (X > 0).astype(float)\n",
    "        dX = mask + (1 - mask) * self.alpha * cp.exp(X)\n",
    "        return upstream * dX\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self, axis=-1):\n",
    "        self.cache = None\n",
    "        self.axis = axis\n",
    "\n",
    "    def forward(self, X):\n",
    "        max_X = cp.max(X, axis=self.axis, keepdims=True)\n",
    "        exp_X = cp.exp(X - max_X)  # 数值稳定性优化\n",
    "        sum_exp = cp.sum(exp_X, axis=self.axis, keepdims=True)\n",
    "        output = exp_X / sum_exp\n",
    "        self.cache = output\n",
    "        return output\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        output = self.cache\n",
    "        axis = self.axis\n",
    "        sum_term = cp.sum(output * upstream, axis=axis, keepdims=True)\n",
    "        return output * (upstream - sum_term)\n",
    "\n",
    "class Swish:\n",
    "    def __init__(self, beta=1.0):\n",
    "        self.cache = (None, None)\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, X):\n",
    "        beta_X = self.beta * X\n",
    "        sigmoid = 1 / (1 + cp.exp(-beta_X))\n",
    "        output = X * sigmoid\n",
    "        self.cache = (X, sigmoid)\n",
    "        return output\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        X, sigmoid = self.cache\n",
    "        beta = self.beta\n",
    "        dX = sigmoid + beta * X * sigmoid * (1 - sigmoid)\n",
    "        return upstream * dX\n",
    "\n",
    "\n",
    "\n",
    "class myNet:\n",
    "    \n",
    "    # 激活函数映射字典，这样子去动态选择激活函数(不需要放在init里面）    \n",
    "    ACTIVATION_FUNCTIONS = {\n",
    "        'relu': ReLU,\n",
    "        'sigmoid': Sigmoid,\n",
    "        'tanh': Tanh,\n",
    "        'leaky_relu': LeakyReLU,\n",
    "        'elu': ELU,\n",
    "        'softmax':Softmax,\n",
    "        'swish': Swish\n",
    "    }\n",
    "        \n",
    "    def __init__(self,input_dim,hidden_dim1,hidden_dim2,output_dim,activation):\n",
    "\n",
    "        #把这些变量保存为实例变量，用于我们等会儿的save操作\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "\n",
    "        \n",
    "        self.activation_class = self.ACTIVATION_FUNCTIONS[activation]\n",
    "        self.layers=[\n",
    "            Linear(input_dim,hidden_dim1),\n",
    "            self.activation_class(),\n",
    "            Linear(hidden_dim1,hidden_dim2),\n",
    "            self.activation_class(),\n",
    "            Linear(hidden_dim2,output_dim)\n",
    "            ]\n",
    "\n",
    "    def forward(self,X):\n",
    "        for layer in self.layers:\n",
    "            X=layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self,upstream):\n",
    "        for layer in reversed(self.layers):\n",
    "            upstream=layer.backward(upstream)#梯度信息都存在layer里面了\n",
    "            # print(f\"Layer {i} grad mean: {np.mean(upstream):.4f}\")  # 调试\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                params.extend([layer.W, layer.b])\n",
    "        return params\n",
    "\n",
    "    def gradients(self):\n",
    "        grads = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                grads.extend([layer.grad_W, layer.grad_b])\n",
    "        return grads\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                layer.grad_W = cp.zeros_like(layer.W)  # 重置为全零\n",
    "                layer.grad_b = cp.zeros_like(layer.b) \n",
    "                \n",
    "    #CuPy 的 save 方法会保留数组的 GPU 属性，避免将数据传回 CPU 再保存   \n",
    "    #为了超参数那个任务，我们的save和load需要能够保存整个网络结构+保存参数，而不仅仅只是保存参数\n",
    "    #网络结构中含有字符串，我们保存为json格式；参数部分全是数字，我们直接使用cupy从GPU存回来\n",
    "    def save_model(self, stage=1):\n",
    "        save_dir = \"parameters_stage1\" if stage == 1 else \"parameters_stage2\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # 保存结构元数据到JSON（支持字符串）\n",
    "        metadata = {\n",
    "            'input_dim': self.input_dim,\n",
    "            'hidden_dim1': self.hidden_dim1,\n",
    "            'hidden_dim2': self.hidden_dim2,\n",
    "            'output_dim': self.output_dim,\n",
    "            'activation': self.activation  # 直接保存字符串\n",
    "        }\n",
    "        with open(os.path.join(save_dir, \"metadata.json\"), 'w') as f:\n",
    "            json.dump(metadata, f)\n",
    "        \n",
    "        # 保存权重（CuPy格式）\n",
    "        for i, layer in enumerate([l for l in self.layers if isinstance(l, Linear)]):\n",
    "            cp.save(os.path.join(save_dir, f\"layer_{i}_weight.npy\"), layer.W)\n",
    "            cp.save(os.path.join(save_dir, f\"layer_{i}_bias.npy\"), layer.b)\n",
    "            \n",
    "    def load_model(self,stage=1):\n",
    "    #cp.save 生成的 .npy 文件格式与 NumPy 兼容，可用 np.load 读取（但会加载到 CPU）\n",
    "    #而我们使用cupy：直接从文件加载到 GPU 内存，保持与模型其他部分（如权重）的 GPU 一致性\n",
    "        \n",
    "        save_dir = \"parameters_stage1\" if stage == 1 else \"parameters_stage2\"\n",
    "        \n",
    "        # 加载结构元数据\n",
    "        with open(os.path.join(save_dir, \"metadata.json\"), 'r') as f:\n",
    "            metadata = json.load(f)  # 直接加载字符串\n",
    "        \n",
    "        # 重建模型结构\n",
    "        self.__init__(\n",
    "            input_dim=metadata['input_dim'],\n",
    "            hidden_dim1=metadata['hidden_dim1'],\n",
    "            hidden_dim2=metadata['hidden_dim2'],\n",
    "            output_dim=metadata['output_dim'],\n",
    "            activation=metadata['activation']\n",
    "        )\n",
    "        \n",
    "        # 加载权重（CuPy格式）\n",
    "        layer_idx = 0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                layer.W = cp.load(os.path.join(save_dir, f\"layer_{layer_idx}_weight.npy\"))\n",
    "                layer.b = cp.load(os.path.join(save_dir, f\"layer_{layer_idx}_bias.npy\"))\n",
    "                layer_idx += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84cc99-15bb-42a6-b5f7-60a335d0d860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-fdu",
   "language": "python",
   "name": "nlp-fdu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
